# @package _global_
alg_name: MELO
model_name: /data2/zhonghaitian/Models/Meta-Llama-3.1-8B-Instruct
model_parallel: false
device: 6
max_length: 512
use_customized: True
loc_whole: True
task: hall
lora_task_type: CAUSAL_LM
#成对出现

check_dir: null

grace:
  name: grace
  num_iter: 50
  init_radius: 0.5
  dist_fn: euc # euc, mmd, cos
  val_init: cold # cold, warm
  val_train: sgd # sgd, pert
  val_reg: None # early
  reg: early_stop # early_stop
  replacement: replace_prompt # replace_last, replace_all, replace_prompt
  expand_mode: moving_avg # , moving_avg, decay
  num_pert: 8 # only matters when using perturbation training
  key_id: -1
  num_edit_per_block: 4
  num_block: 350
  num_rank_per_block: 2
  metric_period: 400
  edit_lr: 0.001
model:
  name: /data2/zhonghaitian/Models/Meta-Llama-3.1-8B-Instruct
  class_name: LlamaForCausalLM
  tokenizer_class: LlamaTokenizerFast
  tokenizer_name: /data2/zhonghaitian/Models/Meta-Llama-3.1-8B-Instruct
  fan_in_fan_out: True
  target_modules:
    - model.layers.31.mlp.up_proj
    - model.layers.30.mlp.up_proj
  pt: ./results/InstructEdit/melo/checkpoint # set this to 'hallucination' inside your checkpoint directory
  grace_layer: model.layers.29.mlp.up_proj
lora:
  cls_name:  ./model/distilbert-base-cased
  cls_class: AutoModel
  supervised: true
  cos: false
  freeze: null
  square: true
  bound_embeds: false
  use_all_negatives: false
  freeze_lora: false
  dist_heads: 1
  cross_attend: false
  soft_weighting: false
  checkpoint_grad: false
  lora_r: 64
  lora_alpha: 64
  lora_dropout: 0.0
